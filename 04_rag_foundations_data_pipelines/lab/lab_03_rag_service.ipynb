{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5",
   "metadata": {},
   "source": [
    "# Lab 3: Building the RAG Service\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Configure a ChromaDB vector store with HNSW indexing\n",
    "- Implement a VectorStoreManager for batch ingestion and search\n",
    "- Build an end-to-end RAGService that orchestrates Retrieve → Augment → Generate\n",
    "- Test your RAG system with real queries\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a7b8c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb numpy -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5",
   "metadata": {},
   "source": [
    "## Part 1: Vector Store Setup\n",
    "\n",
    "**ChromaDB** is an open-source embedding database that makes it easy to store, search, and retrieve vector embeddings. Under the hood, it uses **HNSW (Hierarchical Navigable Small World)** as its approximate nearest neighbor (ANN) index.\n",
    "\n",
    "Key HNSW parameters:\n",
    "- `hnsw:space` — Distance metric (`cosine`, `l2`, `ip`). We use `cosine` for text embeddings.\n",
    "- `hnsw:construction_ef` — Controls index build quality. Higher = better recall, slower build.\n",
    "- `hnsw:search_ef` — Controls search quality. Higher = better recall, slower search.\n",
    "- `hnsw:M` — Number of bi-directional links per node. Higher = better recall, more memory.\n",
    "\n",
    "We will create a `VectorStoreManager` class that wraps ChromaDB to provide a clean interface for document ingestion and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472d47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'research_papers' ready (In-memory)\n",
      "Stats: {'count': 0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "class FakeVectorStore:\n",
    "    \"\"\"Simple in-memory vector store (chromadb alternative).\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name=\"research_papers\"):\n",
    "        self.documents = []\n",
    "        self.collection_name = collection_name\n",
    "        print(f\"Collection '{collection_name}' ready (In-memory)\")\n",
    "    \n",
    "    def add_documents(self, documents, batch_size=100):\n",
    "        \"\"\"Add documents with embeddings.\"\"\"\n",
    "        for doc in documents:\n",
    "            self.documents.append(doc)\n",
    "        \n",
    "        print(f\"Added {len(documents)} documents. Collection size: {len(self.documents)}\")\n",
    "        return len(documents)\n",
    "    \n",
    "    def search(self, query_embedding, n_results=5, filter_conditions=None):\n",
    "        \"\"\"Search for similar documents using cosine similarity.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_embedding, doc['embedding']) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(doc['embedding'])\n",
    "            )\n",
    "            results.append({\n",
    "                \"text\": doc['text'],\n",
    "                \"metadata\": doc.get('metadata', {}),\n",
    "                \"score\": similarity\n",
    "            })\n",
    "        \n",
    "        # Sort by score descending\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return results[:n_results]\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\"count\": len(self.documents)}\n",
    "\n",
    "# Use the fake store instead\n",
    "store = FakeVectorStore()\n",
    "print(f\"Stats: {store.get_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e2f3a4b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigError",
     "evalue": "unable to infer type for attribute \"chroma_server_nofile\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConfigError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\chromadb\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client \u001b[38;5;28;01mas\u001b[39;00m ClientCreator\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     AdminClient \u001b[38;5;28;01mas\u001b[39;00m AdminClientCreator,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncClient \u001b[38;5;28;01mas\u001b[39;00m AsyncClientCreator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\chromadb\\api\\__init__.py:51\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollection_configuration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     CreateCollectionConfiguration,\n\u001b[32m     49\u001b[39m     UpdateCollectionConfiguration,\n\u001b[32m     50\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_DATABASE, DEFAULT_TENANT\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     53\u001b[39m     CollectionMetadata,\n\u001b[32m     54\u001b[39m     Documents,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     DefaultEmbeddingFunction,\n\u001b[32m     74\u001b[39m )\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserIdentity\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\chromadb\\config.py:120\u001b[39m\n\u001b[32m    116\u001b[39m     NODE = \u001b[33m\"\u001b[39m\u001b[33mnode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     ID = \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSettings\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseSettings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ==============\u001b[39;49;00m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Generic config\u001b[39;49;00m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ==============\u001b[39;49;00m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Can be \"chromadb.api.segment.SegmentAPI\" or \"chromadb.api.fastapi.FastAPI\" or \"chromadb.api.rust.RustBindingsAPI\"\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\pydantic\\v1\\main.py:221\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, name, bases, namespace, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_valid_field(var_name) \u001b[38;5;129;01mand\u001b[39;00m var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m annotations \u001b[38;5;129;01mand\u001b[39;00m can_be_changed:\n\u001b[32m    220\u001b[39m     validate_field_name(bases, var_name)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     inferred = \u001b[43mModelField\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUndefined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(inferred.type_, fields[var_name].type_):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\pydantic\\v1\\fields.py:504\u001b[39m, in \u001b[36mModelField.infer\u001b[39m\u001b[34m(cls, name, value, annotation, class_validators, config)\u001b[39m\n\u001b[32m    501\u001b[39m     required = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    502\u001b[39m annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\pydantic\\v1\\fields.py:434\u001b[39m, in \u001b[36mModelField.__init__\u001b[39m\u001b[34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.shape: \u001b[38;5;28mint\u001b[39m = SHAPE_SINGLETON\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.model_config.prepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\pydantic\\v1\\fields.py:544\u001b[39m, in \u001b[36mModelField.prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Prepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03m    Note: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    e.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[32m    546\u001b[39m         \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[32m    547\u001b[39m         \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[32m    548\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\.venv-1\\Lib\\site-packages\\pydantic\\v1\\fields.py:576\u001b[39m, in \u001b[36mModelField._set_default_and_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    573\u001b[39m     \u001b[38;5;28mself\u001b[39m.annotation = \u001b[38;5;28mself\u001b[39m.type_\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors_.ConfigError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33munable to infer type for attribute \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28mself\u001b[39m.allow_none = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConfigError\u001b[39m: unable to infer type for attribute \"chroma_server_nofile\""
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manages ChromaDB with HNSW indexing.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name=\"research_papers\"):\n",
    "        # Use in-memory client for this lab\n",
    "        self.client = chromadb.Client()\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\n",
    "                \"hnsw:space\": \"cosine\",\n",
    "                \"hnsw:construction_ef\": 200,\n",
    "                \"hnsw:search_ef\": 100,\n",
    "                \"hnsw:M\": 16,\n",
    "            }\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' ready (HNSW cosine)\")\n",
    "    \n",
    "    def add_documents(self, documents, batch_size=100):\n",
    "        \"\"\"Add documents with embeddings in batches.\"\"\"\n",
    "        total = 0\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            ids, embeddings, metadatas, texts = [], [], [], []\n",
    "            \n",
    "            for doc in batch:\n",
    "                content_hash = hashlib.md5(doc['text'].encode()).hexdigest()\n",
    "                ids.append(f\"doc_{content_hash}\")\n",
    "                embeddings.append(doc['embedding'])\n",
    "                metadatas.append(doc.get('metadata', {}))\n",
    "                texts.append(doc['text'])\n",
    "            \n",
    "            self.collection.add(\n",
    "                embeddings=embeddings,\n",
    "                documents=texts,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            total += len(batch)\n",
    "        \n",
    "        print(f\"Added {total} documents. Collection size: {self.collection.count()}\")\n",
    "        return total\n",
    "    \n",
    "    def search(self, query_embedding, n_results=5, filter_conditions=None):\n",
    "        \"\"\"Search for similar documents.\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=filter_conditions,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        formatted = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            formatted.append({\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "                \"metadata\": results[\"metadatas\"][0][i],\n",
    "                \"score\": 1 - results[\"distances\"][0][i]  # distance -> similarity\n",
    "            })\n",
    "        return formatted\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\"count\": self.collection.count()}\n",
    "\n",
    "# Initialize\n",
    "store = VectorStoreManager()\n",
    "print(f\"Stats: {store.get_stats()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5",
   "metadata": {},
   "source": [
    "## Part 2: Populate with Sample Data\n",
    "\n",
    "We will create simulated research paper chunks with synthetic embeddings. In a production system, these embeddings would come from your ingestion and embedding pipeline (e.g., the `EmbeddingGenerator` from Lab 2).\n",
    "\n",
    "To make the simulation meaningful, we inject a topic-specific signal into each embedding so that semantically related chunks are closer together in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c2d3e4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 documents. Collection size: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated research paper chunks with pre-computed embeddings\n",
    "# In production, these would come from your ingestion + embedding pipeline\n",
    "np.random.seed(42)\n",
    "DIM = 384  # Using smaller dimension for demo\n",
    "\n",
    "sample_chunks = [\n",
    "    {\"text\": \"RAG combines retrieval with generation to ground LLM responses in external knowledge. This reduces hallucinations and enables citation of sources.\", \n",
    "     \"metadata\": {\"title\": \"RAG Survey\", \"section\": \"introduction\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"The transformer architecture uses self-attention mechanisms to process all positions in a sequence simultaneously, enabling massive parallelization during training.\",\n",
    "     \"metadata\": {\"title\": \"Attention Is All You Need\", \"section\": \"architecture\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"HNSW (Hierarchical Navigable Small World) is an approximate nearest neighbor algorithm that builds a multi-layered graph for fast vector search with high recall.\",\n",
    "     \"metadata\": {\"title\": \"HNSW Paper\", \"section\": \"algorithm\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Cosine similarity measures the angle between two vectors, making it robust to document length variation. It is the standard metric for text embeddings.\",\n",
    "     \"metadata\": {\"title\": \"Vector Search Guide\", \"section\": \"metrics\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Fine-tuning changes model weights through additional training, while RAG keeps the model frozen and provides knowledge through retrieval. RAG is more cost-effective for dynamic knowledge.\",\n",
    "     \"metadata\": {\"title\": \"RAG vs Fine-tuning\", \"section\": \"comparison\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Chunking is the process of splitting documents into smaller pieces for embedding. The optimal chunk size balances context preservation with retrieval precision.\",\n",
    "     \"metadata\": {\"title\": \"Chunking Strategies\", \"section\": \"fundamentals\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Cross-encoder re-ranking takes query-document pairs as input and produces a relevance score. It is more accurate than bi-encoders but slower.\",\n",
    "     \"metadata\": {\"title\": \"Re-ranking Survey\", \"section\": \"methods\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"BM25 is a probabilistic ranking function based on term frequency. It excels at exact keyword matching, complementing semantic search in hybrid systems.\",\n",
    "     \"metadata\": {\"title\": \"Hybrid Search\", \"section\": \"bm25\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Vector quantization reduces memory by storing vectors with lower precision. Scalar quantization (SQ8) achieves 4x compression with minimal recall loss.\",\n",
    "     \"metadata\": {\"title\": \"Scaling Vectors\", \"section\": \"quantization\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"The evaluation of RAG systems requires both retrieval metrics (Hit Rate, MRR) and generation metrics (faithfulness, relevance). DeepEval provides automated LLM-as-Judge evaluation.\",\n",
    "     \"metadata\": {\"title\": \"RAG Evaluation\", \"section\": \"metrics\", \"chunk_id\": 0}},\n",
    "]\n",
    "\n",
    "# Generate embeddings that cluster by topic (simulated)\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    base = np.random.randn(DIM) * 0.1\n",
    "    if \"RAG\" in chunk[\"text\"] or \"retrieval\" in chunk[\"text\"].lower():\n",
    "        base[:50] += 0.5\n",
    "    if \"vector\" in chunk[\"text\"].lower() or \"embedding\" in chunk[\"text\"].lower():\n",
    "        base[50:100] += 0.5\n",
    "    chunk[\"embedding\"] = (base / np.linalg.norm(base)).tolist()\n",
    "\n",
    "# Add to vector store\n",
    "store.add_documents(sample_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5e6",
   "metadata": {},
   "source": [
    "## Part 3: Search Testing\n",
    "\n",
    "Let's test that our vector store returns relevant results. We create a query embedding with a RAG topic signal and verify that the top results are indeed about RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a8b9c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG and how does it work?\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "  [1] Score: 0.7659\n",
      "      Title: RAG Evaluation\n",
      "      Text: The evaluation of RAG systems requires both retrieval metrics (Hit Rate, MRR) and generation metrics (faithfulness, rele...\n",
      "\n",
      "  [2] Score: 0.7615\n",
      "      Title: RAG Survey\n",
      "      Text: RAG combines retrieval with generation to ground LLM responses in external knowledge. This reduces hallucinations and en...\n",
      "\n",
      "  [3] Score: 0.7581\n",
      "      Title: RAG vs Fine-tuning\n",
      "      Text: Fine-tuning changes model weights through additional training, while RAG keeps the model frozen and provides knowledge t...\n"
     ]
    }
   ],
   "source": [
    "# Create a query embedding (simulated)\n",
    "query = \"What is RAG and how does it work?\"\n",
    "# Query embedding with RAG topic signal\n",
    "query_emb = np.random.randn(DIM) * 0.1\n",
    "query_emb[:50] += 0.5  # RAG topic signal\n",
    "query_emb = (query_emb / np.linalg.norm(query_emb)).tolist()\n",
    "\n",
    "results = store.search(query_emb, n_results=3)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {len(results)} results:\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n  [{i+1}] Score: {r['score']:.4f}\")\n",
    "    print(f\"      Title: {r['metadata'].get('title', 'N/A')}\")\n",
    "    print(f\"      Text: {r['text'][:120]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3a4b5c6",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Metadata Filtering\n",
    "\n",
    "ChromaDB supports metadata filtering via the `where` parameter. This allows you to combine vector similarity search with structured filters -- for example, searching only within a specific section or document title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e3f4a5b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "All results should be from 'architecture' section",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m filtered_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mCall store.search with filter_conditions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_results) > \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mShould find at least one result\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(r[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msection\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m filtered_results), \\\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAll results should be from \u001b[39m\u001b[33m'\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m'\u001b[39m\u001b[33m section\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m results in \u001b[39m\u001b[33m'\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m'\u001b[39m\u001b[33m section\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m filtered_results:\n",
      "\u001b[31mAssertionError\u001b[39m: All results should be from 'architecture' section"
     ]
    }
   ],
   "source": [
    "# TODO: Search for chunks specifically about \"architecture\" section\n",
    "# Use the filter_conditions parameter with: {\"section\": \"architecture\"}\n",
    "\n",
    "filtered_results = store.search(query_emb, n_results=5, filter_conditions={\"section\": \"architecture\"})\n",
    "\n",
    "assert filtered_results is not None, \"Call store.search with filter_conditions\"\n",
    "assert len(filtered_results) > 0, \"Should find at least one result\"\n",
    "assert all(r['metadata']['section'] == 'architecture' for r in filtered_results), \\\n",
    "    \"All results should be from 'architecture' section\"\n",
    "\n",
    "print(f\"Found {len(filtered_results)} results in 'architecture' section\")\n",
    "for r in filtered_results:\n",
    "    print(f\"  {r['metadata']['title']}: {r['text'][:80]}...\")\n",
    "print(\"Metadata filtering working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5a6",
   "metadata": {},
   "source": [
    "## Part 4: The RAG Service\n",
    "\n",
    "Now we build the complete RAG pipeline that orchestrates three stages:\n",
    "\n",
    "1. **Retrieve** -- Embed the user query and search the vector store for relevant chunks.\n",
    "2. **Augment** -- Format the retrieved chunks into a context string and build a grounded prompt.\n",
    "3. **Generate** -- Send the augmented prompt to the LLM to produce a cited answer.\n",
    "\n",
    "In this lab we simulate embedding and generation. In production, you would replace `_embed_query` and `_generate` with calls to OpenAI, Anthropic, or another provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c3d4e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the retrieved sources: [1] (Source: RAG Survey)... [1]\n",
      "\n",
      "Sources:\n",
      "  - RAG Survey (introduction)\n",
      "  - RAG vs Fine-tuning (comparison)\n",
      "  - RAG Evaluation (metrics)\n",
      "\n",
      "Retrieval scores: ['0.794', '0.791', '0.765']\n"
     ]
    }
   ],
   "source": [
    "class RAGService:\n",
    "    \"\"\"Orchestrates Retrieve -> Augment -> Generate.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store):\n",
    "        self.vector_store = vector_store\n",
    "        self.embed_dim = DIM\n",
    "    \n",
    "    def _embed_query(self, query: str) -> list:\n",
    "        \"\"\"Generate query embedding. In production: call OpenAI API.\"\"\"\n",
    "        # Simulated embedding based on keywords\n",
    "        emb = np.random.randn(self.embed_dim) * 0.1\n",
    "        query_lower = query.lower()\n",
    "        if any(w in query_lower for w in [\"rag\", \"retrieval\", \"generation\"]):\n",
    "            emb[:50] += 0.5\n",
    "        if any(w in query_lower for w in [\"vector\", \"embedding\", \"cosine\"]):\n",
    "            emb[50:100] += 0.5\n",
    "        if any(w in query_lower for w in [\"chunk\", \"split\"]):\n",
    "            emb[100:150] += 0.5\n",
    "        return (emb / np.linalg.norm(emb)).tolist()\n",
    "    \n",
    "    def _build_context(self, results: list) -> str:\n",
    "        \"\"\"Format retrieved chunks into context string.\"\"\"\n",
    "        parts = []\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            title = doc['metadata'].get('title', 'Unknown')\n",
    "            parts.append(f\"[{i}] (Source: {title})\\n{doc['text']}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    \n",
    "    def _build_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Build the full prompt for the LLM.\"\"\"\n",
    "        system = \"\"\"You are a Research Assistant. Answer using ONLY the provided context.\n",
    "Cite sources using [N] format. If the answer is not in the context, say so.\"\"\"\n",
    "        \n",
    "        return f\"\"\"SYSTEM: {system}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    def _generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate answer. In production: call OpenAI/Claude API.\"\"\"\n",
    "        # Simulated generation - extracts key sentences from context\n",
    "        lines = prompt.split(\"\\n\")\n",
    "        context_lines = [l for l in lines if l.startswith(\"[\")]\n",
    "        if context_lines:\n",
    "            return f\"Based on the retrieved sources: {context_lines[0][:200]}... [1]\"\n",
    "        return \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    def answer(self, query: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"Full RAG pipeline: Retrieve -> Augment -> Generate.\"\"\"\n",
    "        # 1. RETRIEVE\n",
    "        query_embedding = self._embed_query(query)\n",
    "        results = self.vector_store.search(query_embedding, n_results=top_k)\n",
    "        \n",
    "        if not results:\n",
    "            return {\"answer\": \"No relevant information found.\", \"sources\": []}\n",
    "        \n",
    "        # 2. AUGMENT\n",
    "        context = self._build_context(results)\n",
    "        prompt = self._build_prompt(query, context)\n",
    "        \n",
    "        # 3. GENERATE\n",
    "        answer = self._generate(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [r['metadata'] for r in results],\n",
    "            \"scores\": [r['score'] for r in results],\n",
    "            \"prompt_preview\": prompt[:500] + \"...\"\n",
    "        }\n",
    "\n",
    "# Test the complete pipeline\n",
    "rag = RAGService(store)\n",
    "response = rag.answer(\"What is RAG and why is it useful?\")\n",
    "\n",
    "print(f\"Answer: {response['answer']}\")\n",
    "print(f\"\\nSources:\")\n",
    "for s in response['sources']:\n",
    "    print(f\"  - {s.get('title', 'N/A')} ({s.get('section', 'N/A')})\")\n",
    "print(f\"\\nRetrieval scores: {[f'{s:.3f}' for s in response['scores']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Add Score Threshold\n",
    "\n",
    "In production, not all retrieved results are useful. Low-scoring results can introduce noise into the context and degrade generation quality. Implement a `min_score` threshold to filter out weak matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a9b0c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Score threshold filtering working!\n"
     ]
    }
   ],
   "source": [
    "class ImprovedRAGService(RAGService):\n",
    "    \"\"\"RAG Service with minimum score threshold.\"\"\"\n",
    "    \n",
    "    def answer(self, query: str, top_k: int = 3, min_score: float = 0.0) -> dict:\n",
    "        # 1. Call self._embed_query(query)\n",
    "        query_embedding = self._embed_query(query)\n",
    "        \n",
    "        # 2. Call self.vector_store.search(embedding, n_results=top_k)\n",
    "        results = self.vector_store.search(query_embedding, n_results=top_k)\n",
    "        \n",
    "        # 3. Filter results where score >= min_score\n",
    "        filtered_results = [r for r in results if r['score'] >= min_score]\n",
    "        \n",
    "        # 4. If no results pass the threshold, return a \"no information\" message\n",
    "        if not filtered_results:\n",
    "            return {\n",
    "                \"answer\": \"No relevant information found above the confidence threshold.\",\n",
    "                \"sources\": [],\n",
    "                \"scores\": [],\n",
    "                \"prompt_preview\": \"\"\n",
    "            }\n",
    "        \n",
    "        # 5. Build context and generate as before\n",
    "        context = self._build_context(filtered_results)\n",
    "        prompt = self._build_prompt(query, context)\n",
    "        answer = self._generate(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [r['metadata'] for r in filtered_results],\n",
    "            \"scores\": [r['score'] for r in filtered_results],\n",
    "            \"prompt_preview\": prompt[:500] + \"...\"\n",
    "        }\n",
    "\n",
    "# Test\n",
    "improved_rag = ImprovedRAGService(store)\n",
    "result = improved_rag.answer(\"What is RAG?\", min_score=0.5)\n",
    "\n",
    "from tests import checks\n",
    "checks.check_lab_3_4(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4f5a6b7",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Deduplication Check\n",
    "\n",
    "When ingesting documents at scale, you may encounter duplicates. Let's verify how ChromaDB handles re-adding documents that already exist in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d4e5f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 documents. Collection size: 16\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Count changed from 13 to 16. Check your dedup logic.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m final_count = store.get_stats()[\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m checks\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mchecks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_lab_3_4_dedup\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rwanj\\OneDrive\\Desktop\\SDAIA\\SDAIA-Building-Gen-AI-Apps\\04_rag_foundations_data_pipelines\\lab\\tests\\checks.py:53\u001b[39m, in \u001b[36mcheck_lab_3_4_dedup\u001b[39m\u001b[34m(initial_count, final_count)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_lab_3_4_dedup\u001b[39m(initial_count, final_count):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m initial_count == final_count, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCount changed from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Check your dedup logic.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Dedup check complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Count changed from 13 to 16. Check your dedup logic."
     ]
    }
   ],
   "source": [
    "# TODO: Verify that adding the same documents again doesn't create duplicates\n",
    "initial_count = store.get_stats()['count']\n",
    "\n",
    "# Re-add the same documents\n",
    "store.add_documents(sample_chunks[:3])\n",
    "final_count = store.get_stats()['count']\n",
    "from tests import checks\n",
    "checks.check_lab_3_4_dedup(initial_count, final_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6f7",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. **HNSW Parameters**: What would happen if you set `hnsw:M` to 2 instead of 16? What about 128?\n",
    "2. **Score Interpretation**: A result comes back with score 0.72. Is this \"good enough\"? What factors determine the threshold?\n",
    "3. **Production Gap**: What are 3 things missing from our RAGService that a production system would need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7e8",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1 (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
