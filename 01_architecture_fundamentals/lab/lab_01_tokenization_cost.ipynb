{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tokenization & Cost Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Understand how LLMs break text into tokens using BPE encoding\n",
    "- Analyze how different text types (code, prose, multilingual) tokenize differently\n",
    "- Estimate API costs based on token counts\n",
    "- Visualize context window evolution and understand O(n²) scaling implications\n",
    "\n",
    "## Setup\n",
    "Run the cell below to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken plotly -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: The Tokenizer\n",
    "\n",
    "Tokens are not characters or words — they're **subword units** learned during training. Let's see how the same text breaks apart differently across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def analyze_tokenization(text):\n",
    "    \"\"\"Demonstrate how text breaks into tokens across different models.\"\"\"\n",
    "    # tiktoken supports OpenAI model tokenizers\n",
    "    encoders = {\n",
    "        \"GPT-4\": tiktoken.encoding_for_model(\"gpt-4\"),\n",
    "        \"GPT-3.5-turbo\": tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for model_name, encoder in encoders.items():\n",
    "        tokens = encoder.encode(text)\n",
    "        decoded_tokens = [encoder.decode([token]) for token in tokens]\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"token_count\": len(tokens),\n",
    "            \"tokens\": decoded_tokens,\n",
    "            \"cost_estimate\": len(tokens) * 0.0000015,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  Code\n",
      "==================================================\n",
      "Text: def calculate_sum(a, b):\n",
      "    return a + b\n",
      "Character count: 41\n",
      "\n",
      "  GPT-4:\n",
      "    Tokens: 12\n",
      "    Breakdown: ['def', ' calculate', '_sum', '(a', ',', ' b', '):\\n', '   ', ' return', ' a', ' +', ' b']\n",
      "    Approx. cost: $0.000018\n",
      "\n",
      "  GPT-3.5-turbo:\n",
      "    Tokens: 12\n",
      "    Breakdown: ['def', ' calculate', '_sum', '(a', ',', ' b', '):\\n', '   ', ' return', ' a', ' +', ' b']\n",
      "    Approx. cost: $0.000018\n",
      "\n",
      "==================================================\n",
      "  Essay\n",
      "==================================================\n",
      "Text: The transformer architecture revolutionized natural language processing.\n",
      "Character count: 72\n",
      "\n",
      "  GPT-4:\n",
      "    Tokens: 9\n",
      "    Breakdown: ['The', ' transformer', ' architecture', ' revolution', 'ized', ' natural', ' language', ' processing', '.']\n",
      "    Approx. cost: $0.000013\n",
      "\n",
      "  GPT-3.5-turbo:\n",
      "    Tokens: 9\n",
      "    Breakdown: ['The', ' transformer', ' architecture', ' revolution', 'ized', ' natural', ' language', ' processing', '.']\n",
      "    Approx. cost: $0.000013\n",
      "\n",
      "==================================================\n",
      "  Multilingual\n",
      "==================================================\n",
      "Text: Bonjour! 你好! Today we're discussing tokens.\n",
      "Character count: 43\n",
      "\n",
      "  GPT-4:\n",
      "    Tokens: 12\n",
      "    Breakdown: ['Bonjour', '!', ' ', '你', '好', '!', ' Today', ' we', \"'re\", ' discussing', ' tokens', '.']\n",
      "    Approx. cost: $0.000018\n",
      "\n",
      "  GPT-3.5-turbo:\n",
      "    Tokens: 12\n",
      "    Breakdown: ['Bonjour', '!', ' ', '你', '好', '!', ' Today', ' we', \"'re\", ' discussing', ' tokens', '.']\n",
      "    Approx. cost: $0.000018\n"
     ]
    }
   ],
   "source": [
    "# Run on three different text types — observe how token counts vary\n",
    "sample_code = \"def calculate_sum(a, b):\\n    return a + b\"\n",
    "sample_essay = \"The transformer architecture revolutionized natural language processing.\"\n",
    "sample_multilingual = \"Bonjour! 你好! Today we're discussing tokens.\"\n",
    "\n",
    "for sample_name, sample_text in [\n",
    "    (\"Code\", sample_code),\n",
    "    (\"Essay\", sample_essay),\n",
    "    (\"Multilingual\", sample_multilingual),\n",
    "]:\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"  {sample_name}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"Text: {sample_text}\")\n",
    "    print(f\"Character count: {len(sample_text)}\")\n",
    "    token_info = analyze_tokenization(sample_text)\n",
    "\n",
    "    for model, info in token_info.items():\n",
    "        print(f\"\\n  {model}:\")\n",
    "        print(f\"    Tokens: {info['token_count']}\")\n",
    "        print(f\"    Breakdown: {info['tokens']}\")\n",
    "        print(f\"    Approx. cost: ${info['cost_estimate']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Tokenize Your Own Samples\n",
    "\n",
    "Choose **3 samples of your own** — try different formats to explore how tokenization varies:\n",
    "- A paragraph from a news article\n",
    "- A JSON or YAML snippet\n",
    "- A sentence mixing Arabic and English\n",
    "\n",
    "Run them through `analyze_tokenization()` and note which produces the most tokens per character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  Sample 1\n",
      "==================================================\n",
      "Text: \n",
      "Apple announced the release of its new iPhone 16 with advanced AI features \n",
      "yes...\n",
      "Characters: 272\n",
      "  GPT-4: 55 tokens (ratio: 0.20 tokens/char)\n",
      "  GPT-3.5-turbo: 55 tokens (ratio: 0.20 tokens/char)\n",
      "\n",
      "==================================================\n",
      "  Sample 2\n",
      "==================================================\n",
      "Text: {\n",
      "    \"user\": {\n",
      "        \"id\": 12345,\n",
      "        \"name\": \"John Smith\",\n",
      "        \"emai...\n",
      "Characters: 239\n",
      "  GPT-4: 61 tokens (ratio: 0.26 tokens/char)\n",
      "  GPT-3.5-turbo: 61 tokens (ratio: 0.26 tokens/char)\n",
      "\n",
      "==================================================\n",
      "  Sample 3\n",
      "==================================================\n",
      "Text: I'm learning العربية and it's challenging but مفيد جداً for my career!\n",
      "Characters: 70\n",
      "  GPT-4: 26 tokens (ratio: 0.37 tokens/char)\n",
      "  GPT-3.5-turbo: 26 tokens (ratio: 0.37 tokens/char)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define your 3 custom samples\n",
    "my_sample_1 = \"\"\"\n",
    "Apple announced the release of its new iPhone 16 with advanced AI features \n",
    "yesterday. The device is expected to retail at $1,200 and will be available \n",
    "in stores starting next month. CEO Tim Cook emphasized the breakthrough \n",
    "camera technology and extended battery life.\n",
    "\"\"\"  # Replace with your first sample\n",
    "my_sample_2 = \"\"\"{\n",
    "    \"user\": {\n",
    "        \"id\": 12345,\n",
    "        \"name\": \"John Smith\",\n",
    "        \"email\": \"john@example.com\",\n",
    "        \"preferences\": {\n",
    "            \"language\": \"en\",\n",
    "            \"notifications\": true,\n",
    "            \"theme\": \"dark\"\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "  # Replace with your second sample\n",
    "my_sample_3 = \"I'm learning العربية and it's challenging but مفيد جداً for my career!\"  # Replace with your third sample\n",
    "\n",
    "for name, text in [\n",
    "    (\"Sample 1\", my_sample_1),\n",
    "    (\"Sample 2\", my_sample_2),\n",
    "    (\"Sample 3\", my_sample_3),\n",
    "]:\n",
    "    if not text:\n",
    "        print(f\"\\n⚠️  {name} is empty — add your text above!\")\n",
    "        continue\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"Text: {text[:80]}{'...' if len(text) > 80 else ''}\")\n",
    "    print(f\"Characters: {len(text)}\")\n",
    "    info = analyze_tokenization(text)\n",
    "    for model, data in info.items():\n",
    "        ratio = data['token_count'] / len(text) if text else 0\n",
    "        print(f\"  {model}: {data['token_count']} tokens (ratio: {ratio:.2f} tokens/char)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Cost Calculator\n",
    "\n",
    "Now let's build a function to estimate real API costs. Here's current pricing data (approximate, as of early 2025):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Pricing (per 1M tokens):\n",
      "Model                     Input     Output\n",
      "------------------------------------------\n",
      "GPT-4-Turbo          $   10.00  $   30.00\n",
      "GPT-4o               $    2.50  $   10.00\n",
      "GPT-3.5-Turbo        $    0.50  $    1.50\n",
      "Claude-3.5-Sonnet    $    3.00  $   15.00\n",
      "Gemini-1.5-Pro       $    1.25  $    5.00\n"
     ]
    }
   ],
   "source": [
    "# Pricing reference (USD per 1M tokens)\n",
    "PRICING = {\n",
    "    \"GPT-4-Turbo\": {\"input\": 10.00, \"output\": 30.00},\n",
    "    \"GPT-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "    \"GPT-3.5-Turbo\": {\"input\": 0.50, \"output\": 1.50},\n",
    "    \"Claude-3.5-Sonnet\": {\"input\": 3.00, \"output\": 15.00},\n",
    "    \"Gemini-1.5-Pro\": {\"input\": 1.25, \"output\": 5.00},\n",
    "}\n",
    "\n",
    "print(\"Model Pricing (per 1M tokens):\")\n",
    "print(f\"{'Model':<20} {'Input':>10} {'Output':>10}\")\n",
    "print(\"-\" * 42)\n",
    "for model, prices in PRICING.items():\n",
    "    print(f\"{model:<20} ${prices['input']:>8.2f}  ${prices['output']:>8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(text, model_name, expected_output_tokens=500):\n",
    "    \"\"\"\n",
    "    Estimate the cost of processing text through a given model.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to be processed\n",
    "        model_name: Key from the PRICING dictionary\n",
    "        expected_output_tokens: Estimated number of output tokens\n",
    "    \n",
    "    Returns:\n",
    "        dict with input_tokens, output_tokens, input_cost, output_cost, total_cost\n",
    "    \n",
    "    Hints:\n",
    "        - Use tiktoken with gpt-4 encoding to count input tokens\n",
    "        - Look up prices from the PRICING dict\n",
    "        - Cost = (token_count / 1_000_000) * price_per_million\n",
    "    \"\"\"\n",
    "    # ✅ Count input tokens using tiktoken (use gpt-4 encoder as a proxy)\n",
    "    encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    input_tokens = len(encoder.encode(text))\n",
    "\n",
    "    # ✅ Look up pricing for the model\n",
    "    prices = PRICING[model_name]\n",
    "\n",
    "    # ✅ Calculate costs\n",
    "    input_cost = (input_tokens / 1_000_000) * prices[\"input\"]\n",
    "    output_cost = (expected_output_tokens / 1_000_000) * prices[\"output\"]\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": expected_output_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": input_cost + output_cost,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Cost Function\n",
    "\n",
    "**Scenario**: Estimate the cost of processing a 10-page document (~5,000 words ≈ ~6,500 tokens) across GPT-4-Turbo vs GPT-3.5-Turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a 10-page document (~5000 words)\n",
    "ten_page_doc = (\"The rapid advancement of artificial intelligence has transformed \"\n",
    "                \"industries across the globe. Organizations are increasingly adopting \"\n",
    "                \"machine learning systems for automation, analysis, and decision-making. \") * 170\n",
    "\n",
    "print(f\"Document length: {len(ten_page_doc)} characters, ~{len(ten_page_doc.split())} words\")\n",
    "print()\n",
    "\n",
    "for model in [\"GPT-4-Turbo\", \"GPT-3.5-Turbo\"]:\n",
    "    result = estimate_cost(ten_page_doc, model, expected_output_tokens=500)\n",
    "    if result[\"total_cost\"] is not None:\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  Input tokens:  {result['input_tokens']:,}\")\n",
    "        print(f\"  Input cost:    ${result['input_cost']:.4f}\")\n",
    "        print(f\"  Output cost:   ${result['output_cost']:.4f}\")\n",
    "        print(f\"  Total cost:    ${result['total_cost']:.4f}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"{model}: ⚠️  Complete the estimate_cost() function above first!\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Context Window Visualization\n",
    "\n",
    "Context windows have grown dramatically. Let's visualize this evolution and understand the computational implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Context window sizes over time\n",
    "models = [\n",
    "    (\"GPT-2\", 2019, 1024),\n",
    "    (\"GPT-3\", 2020, 4096),\n",
    "    (\"GPT-3.5\", 2022, 4096),\n",
    "    (\"Claude 1\", 2023, 9000),\n",
    "    (\"GPT-4\", 2023, 8192),\n",
    "    (\"GPT-4-32K\", 2023, 32768),\n",
    "    (\"Claude 2\", 2023, 100000),\n",
    "    (\"GPT-4-Turbo\", 2023, 128000),\n",
    "    (\"Gemini 1.5 Pro\", 2024, 1000000),\n",
    "    (\"Claude 3.5\", 2024, 200000),\n",
    "]\n",
    "\n",
    "names = [m[0] for m in models]\n",
    "years = [m[1] for m in models]\n",
    "sizes = [m[2] for m in models]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=names,\n",
    "    y=sizes,\n",
    "    marker_color=[\"#9B8EC0\" if y < 2023 else \"#00C9A7\" if y == 2023 else \"#FF7A5C\" for y in years],\n",
    "    text=[f\"{s:,}\" for s in sizes],\n",
    "    textposition=\"outside\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Context Window Evolution (tokens)\",\n",
    "    yaxis_title=\"Max Context Length (tokens)\",\n",
    "    yaxis_type=\"log\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_computation_requirements(context_length, batch_size=1):\n",
    "    \"\"\"\n",
    "    Illustrate how computational requirements scale with context length.\n",
    "    These numbers show relative growth patterns, not actual hardware specs.\n",
    "    \"\"\"\n",
    "    attention_computations = context_length ** 2\n",
    "    memory_gb = (attention_computations * 4 * 8) / (1024**3)  # float32, 8 heads\n",
    "    latency_ms = attention_computations / 1_000_000\n",
    "\n",
    "    return {\n",
    "        \"context_tokens\": context_length,\n",
    "        \"attention_computations\": f\"{attention_computations:,}\",\n",
    "        \"estimated_memory_gb\": round(memory_gb, 2),\n",
    "        \"approximate_latency_ms\": round(latency_ms, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "context_lengths = [1024, 4096, 8192, 32768, 128000]\n",
    "print(\"Computation Requirements by Context Length (O(n²) scaling):\")\n",
    "print(\"-\" * 72)\n",
    "print(f\"{'Tokens':>10}  {'Computations':>18}  {'Memory (GB)':>12}  {'Latency (ms)':>14}\")\n",
    "print(\"-\" * 72)\n",
    "for length in context_lengths:\n",
    "    reqs = estimate_computation_requirements(length)\n",
    "    print(\n",
    "        f\"{length:>10,}  {reqs['attention_computations']:>18}  \"\n",
    "        f\"{reqs['estimated_memory_gb']:>12}  {reqs['approximate_latency_ms']:>14}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "Answer these in a markdown cell below (double-click to edit):\n",
    "\n",
    "1. **Token efficiency**: Which text format produced the fewest tokens per character? Why do you think that is?\n",
    "2. **O(n²) scaling**: If you double the context window, by what factor do attention computations increase? What does this mean for real-time applications?\n",
    "3. **Practical choice**: When would you choose a smaller context window (e.g., 4K) even though a larger one (128K) is available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "\n",
    "1.  Token efficiency:\n",
    "   English essay had the fewest tokens/char (~0.11) because English words \n",
    "   tokenize efficiently. JSON had the most (~0.26) due to brackets, quotes, \n",
    "   and symbols each being separate tokens.\n",
    "2.  O(n²) scaling:\n",
    "   Doubling context = 4x more computations (2² = 4).\n",
    "   Example: 4K→8K went from 16M to 67M operations.\n",
    "   Result: Larger contexts are much slower and more expensive - not suitable \n",
    "   for real-time apps like chatbots.\n",
    "3. Practical choice:\n",
    "   Use 4K when you need speed (chatbots, autocomplete) or low cost \n",
    "   (high-traffic apps). Use 128K only when truly needed (analyzing \n",
    "   full documents, research papers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Token Count Comparison Across Formats\n",
    "\n",
    "Create a bar chart comparing token counts for the **same information** expressed in different formats: English prose, Python code, JSON, and Arabic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Same information in different formats\n",
    "formats = {\n",
    "    \"English\": \"The user's name is Ahmed and he is 30 years old and lives in Riyadh.\",\n",
    "    \"Python\": 'user = {\"name\": \"Ahmed\", \"age\": 30, \"city\": \"Riyadh\"}',\n",
    "    \"JSON\": '{\"name\": \"Ahmed\", \"age\": 30, \"city\": \"Riyadh\"}',\n",
    "    \"Arabic\": \"اسم المستخدم أحمد وعمره 30 سنة ويسكن في الرياض.\",\n",
    "}\n",
    "\n",
    "labels = list(formats.keys())\n",
    "token_counts = [len(encoder.encode(text)) for text in formats.values()]\n",
    "char_counts = [len(text) for text in formats.values()]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name=\"Tokens\",\n",
    "    x=labels,\n",
    "    y=token_counts,\n",
    "    marker_color=\"#1C355E\",\n",
    "    text=token_counts,\n",
    "    textposition=\"outside\",\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    name=\"Characters\",\n",
    "    x=labels,\n",
    "    y=char_counts,\n",
    "    marker_color=\"#00C9A7\",\n",
    "    text=char_counts,\n",
    "    textposition=\"outside\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Token vs Character Counts Across Formats\",\n",
    "    yaxis_title=\"Count\",\n",
    "    barmode=\"group\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Print the ratio\n",
    "print(\"\\nTokens per character ratio:\")\n",
    "for label, tc, cc in zip(labels, token_counts, char_counts):\n",
    "    print(f\"  {label}: {tc/cc:.2f} tokens/char ({tc} tokens, {cc} chars)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1 (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
